---
title: "EEA - TP 2"
author: "Nuñez, Nicolas - Rodriguez, Alejandro"
output:
  html_notebook:
    theme: spacelab
    toc: yes
    toc_float: yes
    df_print: paged
  html_document:
    toc: yes
    df_print: paged
---
```{r theme general}
theme <- theme(text = element_text(size=10),
               plot.title = element_text(size = 12, face = "bold.italic", hjust = 0.5), 
               axis.title.x = element_text(size = 10, face="bold", colour='black'),         
               axis.title.y = element_text(size = 10, face="bold"),
               panel.border = element_blank(),
               panel.grid.major = element_blank(),
               panel.grid.minor = element_blank(), 
               legend.title = element_text(face="bold"))
```

```{r, warning=F, message=FALSE}
library(tidyverse)
library(tidymodels)
library(rsample)
library(ggplot2)
library(GGally)
library(corrr)
library(MASS)
library(fpp2)
library(data.table)
library(stringr)
library(dplyr)
library(prophet)
library(tseries)
library(urca)
library(aTSA)
```

# Importación de datasets
El primer paso fue importar los datasets y acomodar los nombres de las columnas para que sea homogéneo en todos los casos
```{r}
# Cargo los datasets
dataset_2019 = fread('./dataset/flujo-vehicular-por-radares-2019.csv', sep=',')
dataset_2020 = fread('./dataset/flujo-vehicular-por-radares-2020.csv', sep=',')
dataset_2021 = fread('./dataset/flujo-vehicular-por-radares-2021.csv', sep=',')
dataset_2022 = fread('./dataset/flujo-vehicular-por-radares-2022.csv', sep=';')

# Ajusto el nombre de las columnas
colnames(dataset_2021) <- c("fecha","hora","autopista_nombre","disp_nombre","disp_ubicacion","seccion_sentido","lat","long","cantidad","hora_diciembre")
colnames(dataset_2022) <- c("fecha","hora","autopista_nombre","disp_nombre","disp_ubicacion","seccion_sentido","lat","long","cantidad")
```

# Exploración de los datos
Realizamos una exploración sobre los datasets y encontramos que los formatos de las fechas varían de año a año, e incluso en un mismo año, algunos meses tienen formato diferente entre sí. Por lo que al evaluar los casos se procedió a realizar un ajuste de los mismos y a generar una nueva variable `fecha_hora` computado como un objeto datatime con la fecha y hora de la medición, el cual será utilizado luego como la unidad de las series de tiempo.

```{r}
unique(dataset_2019$fecha)
unique(dataset_2020$fecha)
unique(dataset_2021$fecha)
unique(dataset_2022$fecha)
```

```{r}
glimpse(dataset_2019)
glimpse(dataset_2020)
glimpse(dataset_2021)
```

Observamos que los registros tienen un sentido, por lo que validamos las combinaciones posibles para la AU 4 Lugones que es nuestro target.

```{r}
unique(dataset_2019[autopista_nombre == "AU 4  Lugones" , seccion_sentido])
```

Vemos que solo tiene un sentido, por lo que no nos preocuparemos en agrupar o filtrar un sentido particular.

# Generación del indice fecha_hora
Una vez que exploramos y verificamos las distintas variantes de fechas, procedemos a generar el id `fecha_hora` en base al análisis previo.

## 2019
El formato de fechas es homogéneo y en todos los casoes es `yyyy-mm-dd`
```{r}
# Genero una variable de tipo datetime
dataset_2019[ , fecha_hora := as.POSIXct(paste(fecha, hora), format="%Y-%m-%d %H") ]
```

## 2020
  1. A partir de Octubre cambió el formato de fecha y está separado por '/' en lugar de '-'
  2. junio tiene como año 2019 en lugar de 2020
  3. agosto agrega la hora en el campo fecha, pero sin valor siempre 00:00:00
  4. octubre el separador es '/'
  5. noviembre y diciembre el formato de fecha cambió yy-mm-dd

```{r}
# A partir de Octubre cambió el formato de fecha y està separado por '/' en lugar de '-'
# junio tiene como año 2019 en lugar de 2020
# agosto agrega la hora en el campo fecha, pero sin valor siempre 00:00:00
# octubre el separador es '/'
# noviembre y diciembre el formato de fecha cambió yy-mm-dd
dataset_2020[ , fecha_hora := as.POSIXct(paste(fecha, hora), format="%Y-%m-%d %H") ]
dataset_2020[ str_split(fecha, "-", simplify = TRUE)[,2] == "06" , fecha_hora := as.POSIXct(paste(str_replace(fecha, "2019", "2020"), hora), format="%Y-%m-%d %H")]
dataset_2020[ str_split(fecha, "-", simplify = TRUE)[,2] == "08" , fecha_hora := as.POSIXct(paste(str_replace(fecha, " 00:00:00", ""), hora), format="%Y-%m-%d %H")]
dataset_2020[ str_split(fecha, "/", simplify = TRUE)[,2] == "10" , fecha_hora := as.POSIXct(paste(fecha, hora), format="%Y/%m/%d %H")]
dataset_2020[ str_split(fecha, "-", simplify = TRUE)[,2] == "11" , fecha_hora := as.POSIXct(paste(fecha, hora), format="%y-%m-%d %H")]
dataset_2020[ str_split(fecha, "-", simplify = TRUE)[,2] == "12" , fecha_hora := as.POSIXct(paste(fecha, hora), format="%y-%m-%d %H")]
```

## 2021
  1. En 2021 el formato de fecha cambió a dd/mm/yy
  2. En noviembre el formato de la fecha cambio a dd/mm/yyyy
  3. En diciembre 2021 se empezó a utilizar otro campo, el cual denominamos hora_diciembre

```{r}
# En 2021 el formato de fecha cambió a dd/mm/yy
# En noviembre el formato de la fecha cambio a dd/mm/yyyy
# En diciembre 2021 se empezó a utilizar otro campo, el cual denominamos hora_diciembre
dataset_2021[ , fecha_hora := as.POSIXct(paste(fecha, hora), format="%d/%m/%y %H") ]
dataset_2021[ str_split(fecha, "/", simplify = TRUE)[,2] == "11" , fecha_hora := as.POSIXct(paste(fecha, hora), format="%d/%m/%Y %H")]
dataset_2021[ str_split(fecha, "/", simplify = TRUE)[,2] == "12" , fecha_hora := as.POSIXct(paste(fecha, hora_diciembre), format="%d/%m/%Y %H")]
```

## 2022
En 2022 la fecha volvió a cambiar a dd/mm/yy

```{r}
# En 2022 la fecha volvió a cambiar a dd/mm/yy
dataset_2022[ , fecha_hora := as.POSIXct(paste(fecha, hora), format="%d/%m/%y %H") ]
```

## Validación de los resultados obtenidos
Lo que buscamos verificar es que se hayan generado todos los registros de `fecha_hora` y que no haya datos faltantes.

```{r}
datos_faltantes_2019 =  dataset_2019 %>%
                        gather(., 
                              key = "variables", 
                              value = "valores") %>% # agrupamos por las variables del set
                        group_by(variables) %>% 
                        summarise(valores_unicos = n_distinct(valores),
                        porcentaje_faltantes = sum(is.na(valores))/nrow(dataset_2019)*100) %>% 
                        arrange(desc(porcentaje_faltantes), valores_unicos) # ordenamos por porcentaje de faltantes y valores unicos
datos_faltantes_2019
```

```{r}
datos_faltantes_2020 =  dataset_2020 %>%
                        gather(., 
                              key = "variables", 
                              value = "valores") %>% # agrupamos por las variables del set
                        group_by(variables) %>% 
                        summarise(valores_unicos = n_distinct(valores),
                        porcentaje_faltantes = sum(is.na(valores))/nrow(dataset_2020)*100) %>% 
                        arrange(desc(porcentaje_faltantes), valores_unicos) # ordenamos por porcentaje de faltantes y valores unicos
datos_faltantes_2020
```

```{r}
datos_faltantes_2021 =  dataset_2021 %>%
                        gather(., 
                              key = "variables", 
                              value = "valores") %>% # agrupamos por las variables del set
                        group_by(variables) %>% 
                        summarise(valores_unicos = n_distinct(valores),
                        porcentaje_faltantes = sum(is.na(valores))/nrow(dataset_2021)*100) %>% 
                        arrange(desc(porcentaje_faltantes), valores_unicos) # ordenamos por porcentaje de faltantes y valores unicos
datos_faltantes_2021
```

```{r}
datos_faltantes_2022 =  dataset_2022 %>%
                        gather(., 
                              key = "variables", 
                              value = "valores") %>% # agrupamos por las variables del set
                        group_by(variables) %>% 
                        summarise(valores_unicos = n_distinct(valores),
                        porcentaje_faltantes = sum(is.na(valores))/nrow(dataset_2022)*100) %>% 
                        arrange(desc(porcentaje_faltantes), valores_unicos) # ordenamos por porcentaje de faltantes y valores unicos
datos_faltantes_2022
```


# ARIMA

El modelo ARIMA consta de las siguientes componentes (que, de hecho, le dan su nombre):

   1. Autorregresiva (AR): asume que el valor de la serie en un determinado instante se corresponde
con la combinación lineal de la función en instantes anteriores (hasta un número máximo
determinado de ellos, llamado “p”), a lo que se adiciona un componente de error aleatorio. Es
decir, la información presente de un evento está relacionada con los valores pasados.
   2. Integración (I): se aplicarán sucesivas diferenciaciones en los casos en que las series muestren
evidencia de no-estacionalidad.
   3. Promedio Móvil (MA, del inglés moving average): asume que el valor observado en un instante
se corresponde con un término de error aleatorio a lo que le adiciona una combinación lineal
de errores aleatorios previos (hasta un número máximo de ellos, llamado “q”).


Antes de generar el modelo ARIMA es necesario analizar la serie temporal y entender cuáles son sus carácteristicas para determiar el tipo de modelo que se debe generar y sus coeficientes (p, d, q).

## Dataset para las series temporales
Una vez que logramos ajustar los datasets originales, realizamos la generación del dataset a utilizar en los modelos, el cual consiste en el flujo total de vehículos para la autopista `AU 4  Lugones` agrupando por el índice computado previamente `fecha_hora` y finalmente ordenamos los registros por el índice para poder utilizarlo como input de las función ts()

```{r}
# Genero el el dataset con las mediciones
lugones_2019 = dataset_2019[ autopista_nombre == "AU 4  Lugones" , sum(cantidad) , by = fecha_hora]
lugones_2020 = dataset_2020[ autopista_nombre == "AU 4  Lugones" , sum(cantidad) , by = fecha_hora]
lugones_2021 = dataset_2021[ autopista_nombre == "AU 4  Lugones" , sum(cantidad) , by = fecha_hora]

dataset_2019_2021 = rbindlist(list(lugones_2019, lugones_2020, lugones_2021))
colnames(dataset_2019_2021) <- c("fecha_hora", "cantidad")

# Finalmente los ordeno, por las dudas
dataset_2019_2021 = dataset_2019_2021[ order(fecha_hora) , ]
```


## Exploración de la serie temporal
Con el dataset generado, se procede a generar el objeto `ts` indicando una `frequency = 8760`, que es la cantidad de horas en un año e indicando `start=c(2019,1)` para que la referencia temporal de inicio sea Enero 2019.

```{r}
data_serie_fecha = ts(dataset_2019_2021$cantidad, frequency = 8760, start=c(2019,1))
autoplot(data_serie_fecha)+
  labs(title = "Serie de tiempo",       
       x = "Tiempo",
       y = "Valor",
       colour = "#00a0dc")+
    theme_bw() 

```

A continuación, se grafica la descomposición de las serie temporal para observar la tendencia `trend`, si presenta o no estacionalidad `seasonal` y por último el ruido o valor random presente en los datos, que figura como `reminders`, el cual es el resultado de restar la tendencia y la estacionalidad a los datos de la serie temporal `data`.


Esta descomposición se basa en métodos elementales: 
  - la tendencia se calcula con una media móvil
  - el efecto estacional se calcula promediando los valores de cada unidad de tiempo para todos los periodos
  - los residuos se obtienen restando a la serie observada las dos componentes anteriores. 

```{r}
# Descomposición de la serie de tiempo. Se almacena en el objeto fit
fit <- decompose(data_serie_fecha, type='additive')
#fit <- decompose(data_serie, type='multiplicative')

# Para graficar esta descomposición volvemos a emplear la funcion autoplot, pero con el objeto fit
autoplot(fit)+
  labs(title = "Descomposición de la serie de tiempo",                   
       x = "Tiempo",
       y = "Valor",
       colour = "Gears")+
    theme_bw()
```

## Transformaciones básicas de una serie
  - En el gráfico anterior se observa que la serie parece ser estacionaria, no se observa una marcada creciente o decreciente en las series, sino que se mantiene en rangos similares a lo largo del tiempo. 
  - La serie presenta una tendencia decreciente y luego creciente, marcado por los meses de la pandemia.
  - Parece haber una estacionalidad levemente marcada (meses de verano o vacaciones).
  
### Estabilización de la varianza
Para estabilizar la variabilidad se suelen tomar logaritmos. Esta transformación funcionará bien cuando la variabilidad sea aproximadamente proporcional al nivel de la serie.

```{r}
plot(log(data_serie_fecha))
```
### Eliminación de tendencia
Una forma sencilla de eliminar una tendencia es diferenciar la serie. Esto significa, considerar la serie de diferencias entre una observación y la anterior en lugar de la serie original. 

$\Delta x_{t} = x_{t} - x_{t-1}$

```{r}
par(mfcol = c(1, 1))
x1 <- diff(data_serie_fecha)
plot(x1, main="Tráfico en AU 4 Lugones. Diferencias Lag.")
```

### Eliminación de estacionalidad

Para eliminar la estacionalidad de una serie mensual se pueden tomar diferencias estacionales de orden 12. Si $x_{t}$ es la serie que queremos desestacionalizar, se trata de calcular $\Delta_{12} x_{t} = x_{t} - x_{t-12}$

Para nuestro caso, las muestras son por hora, por lo que intentaremos ver la estacionalidad semanal, por lo que probaremos un lag = 168 (7 * 24), correspondiente a la semana anterior

```{r}
par(mfcol = c(1, 1))
x1_168 <- diff(x1, 168)
plot(x1_168, main="Tráfico en AU 4 Lugones. Diferencias Lag Semanal")
```


## Autocorrelaciones simple y parcial

A continuación, veremos el correlograma de la serie, el cual es una representación gráfica de las autocorrelaciones $\rho (k)$, es decir, las correlaciones entre $x_{t}$ y $x_{t+k}$, en función de $k$:


```{r}
par(mfcol = c(1, 2))
acf(data_serie_fecha) 
pacf(data_serie_fecha)
```
```{r}
par(mfcol = c(1, 2))
acf(x1_168) 
pacf(x1_168)
```

### Generación del modelo

Los valores de `ACF` van de -1 a 1. Un valor próximo a 1 indica una gran correlación entre intervalos, si es próximo a -1 la correlación es inversa, la medición actual tiende a subir cuando la anterior baja, y uno próximo a 0 significa que las columnas comparadas son independientes = no podemos predecirlos.

La gráfica `PACF` es la derivada o pendiente de la `ACF` y nos indica la correlación parcial entre los intervalos, descontando el efecto del resto.

Resulta que estas gráficas nos pueden dar los órdenes para ajustar un modelo ARIMA, la clave está en interpretarlas según la siguiente tabla:

|   |AR(p)|MA(q)|ARMA(p,q)|
|---|---|---|---|
|ACF|varios puntos con coef>0 decayendo|0 excepto los q primeros	|varios puntos con coef>0 decayendo|
|PACF|0 excepto los p primeros	|varios puntos con coef>0	|varios puntos con coef>0|

Como regla, la `PACF` define el orden de `AR(p)` y la `ACF` el orden de `MA(q)`.

Finalmente, nos falta obtener el orde $d$ el cual lo podemos calcular facilmente con la función `ndiffs()`

```{r}
ndiffs(data_serie_fecha)
ndiffs(x1_168)
```

Con los resultados de los gráficos de correlación, podemos intentar generar un modelo 

  - dataset original: ARIMA(4,1,3)
  - lag 168: ARIMA(4,0,4)

```{r}
modelo_arima_413 <- Arima(data_serie_fecha, order = c(4,1,3))
summary(modelo_arima_413)
```

Con la función `forecast` del package `forecast` podemos realizar la predicción del `horizon` deseado, en nuestro caso, estaremos intentando estimar los próximos 360 intervalos que corresponden a las próximas 2 semanas.

```{r}
# elaborando el pronostico
modelo_arima_112_fc <- forecast::forecast(modelo_arima_413, h=360)

# graficando el pronóstico
autoplot(modelo_arima_112_fc)
```

```{r}
# verificando los residuales
checkresiduals(modelo_arima_112_fc)
```

```{r}
modelo_arima_404 <- Arima(x1_168, order = c(4,0,4))
summary(modelo_arima_404)
```



```{r}
# elaborando el pronostico
modelo_arima_404_fc <- forecast::forecast(modelo_arima_404, h=360)

# graficando el pronóstico
autoplot(modelo_arima_404_fc)
```

```{r}
# verificando los residuales
checkresiduals(modelo_arima_404_fc)
```

## Auto ARIMA

Existe una forma simplificada de realizar los pasos anteriores y es a través de la función `auto.arima()`


```{r}
modelo_auto_arima <- auto.arima(data_serie_fecha)

summary(modelo_auto_arima)
```


```{r}
modelo_auto_arima_168 <- auto.arima(x1_168)

summary(modelo_auto_arima_168)
```

Podemos observar que los resultados obtenidos son los mismos que habíamos estimado observado los gráficos de correlación.